---
layout: single
title: "Histories of Deep Learning"
tags: tech machine-learning google facebook
---

[Andrey Kurenkov](http://www.andreykurenkov.com) [wrote](http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning/) a [four](http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning-part-2/)-[part](http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning-part-3/) [series](http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning-part-4/) on deep learning that provides an excellent overview of the field from the mid-20th century through the dramatic improvements that have occurred since 2006. Although this series was written in late 2015, I still find it very valuable.

In May, 2015, Yann LeCun, Yoshua Bengio, and Geoffrey Hinton co-authored [a review article in Nature](http://www.nature.com/nature/journal/v521/n7553/full/nature14539.html). They are three of the most well-known names in artificial intelligence research. [LeCun](https://en.wikipedia.org/wiki/Yann_LeCun) is at NYU and heads [FAIR](https://research.fb.com/people/lecun-yann/) at Facebook, [Hinton](https://en.wikipedia.org/wiki/Geoffrey_Hinton) is at University of Toronto and Google, and [Bengio](http://www.iro.umontreal.ca/~bengioy/yoshua_en/) teaches at Université de Montréal. Along with Ian Goodfellow and Aaron Courville, [Bengio](https://en.wikipedia.org/wiki/Yoshua_Bengio) co-wrote [my favorite single resource on deep learning](http://www.deeplearningbook.org). 

Also interesting to read [Jürgen Schmidhuber's](https://en.wikipedia.org/wiki/Jürgen_Schmidhuber) [critique](http://people.idsia.ch/~juergen/deep-learning-conspiracy.html) of the Nature review article. Schmidhuber co-directs the [Dalle Molle Institute for Artificial Intelligence Research](https://en.wikipedia.org/wiki/Dalle_Molle_Institute_for_Artificial_Intelligence_Research) and is another major figure in AI who is famous for creating [LSTM](http://people.idsia.ch/~juergen/rnn.html). Schmidhuber has been [critical of histories that leave out the contributions](https://www.nytimes.com/2016/11/27/technology/artificial-intelligence-pioneer-jurgen-schmidhuber-overlooked.html) of other pioneers in AI.

Finally, [Haohan Wang](https://www.cs.cmu.edu/~haohanw/), [Bhiksha Raj](https://www.lti.cs.cmu.edu/news/bhiksha-raj-named-ieee-fellow), and [Eric P. Xing](http://www.cs.cmu.edu/~epxing/) at [Carnegie Mellon University](https://ai.cs.cmu.edu) wrote a more advanced perspective on the evolution of Deep Learning. This paper requires a bit more math background. I especially appreciate that section 7 explores optimization of neural networks beyond "vanilla backpropagation", esp. techniques such as rprop, AdaGrad, AdaDelta, and Adaptive Moment Estimation (ADAM). It also has a concise description of the powerful [dropout regularization technique](https://en.wikipedia.org/wiki/Convolutional_neural_network#Dropout). The original dropout paper by Geoffrey Hinton, Nitish Srivastava, et alia just appeared three years ago and is already one of the most cited deep learning papers–[see PDF here](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf).
