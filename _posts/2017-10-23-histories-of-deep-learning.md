---
layout: single
title: "Histories of Deep Learning"
tags: tech machine-learning google facebook microsoft
---

In late 2015, [Andrey Kurenkov](http://www.andreykurenkov.com) wrote a [four](http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning/)-[part](http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning-part-2/) [series](http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning-part-3/) on [deep learning](http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning-part-4/) that provides an excellent overview of the field from the 1950's through the early 21st-century with an emphasis on the dramatic improvements since 2006.

Earlier in 2015, Yann LeCun, Yoshua Bengio, and Geoffrey Hinton published [a review article in Nature](http://www.nature.com/nature/journal/v521/n7553/full/nature14539.html). They are three of the most well-known names in artificial intelligence research. [LeCun](https://en.wikipedia.org/wiki/Yann_LeCun) divides his time between NYU and [Facebook](https://research.fb.com/people/lecun-yann/), [Hinton](https://en.wikipedia.org/wiki/Geoffrey_Hinton) is at University of Toronto and [Google](https://research.google.com/pubs/GeoffreyHinton.html), and [Bengio](http://www.iro.umontreal.ca/~bengioy/yoshua_en/) teaches at Université de Montréal. Bengio advised [Maluuba](https://techcrunch.com/2017/01/18/microsoft-to-double-its-montreal-ai-rd-office-and-invest-7m-in-academic-research/), [an AI startup acquired by Microsoft](https://www.wired.com/story/inside-microsofts-ai-comeback/) in 2017. Along with Ian Goodfellow and Aaron Courville, [Bengio](https://en.wikipedia.org/wiki/Yoshua_Bengio) co-authored [my favorite single resource](http://www.deeplearningbook.org) on deep learning. 

It's also useful to read [Jürgen Schmidhuber's](https://en.wikipedia.org/wiki/Jürgen_Schmidhuber) [critique](http://people.idsia.ch/~juergen/deep-learning-conspiracy.html) of LHB's article in Nature. Schmidhuber co-directs the [Dalle Molle Institute for Artificial Intelligence Research](https://en.wikipedia.org/wiki/Dalle_Molle_Institute_for_Artificial_Intelligence_Research) and is another major figure in AI who is famous for creating [LSTM](http://people.idsia.ch/~juergen/rnn.html). Schmidhuber has been [critical of histories that leave out the contributions](https://www.nytimes.com/2016/11/27/technology/artificial-intelligence-pioneer-jurgen-schmidhuber-overlooked.html) of other pioneers in AI.

Finally, [Haohan Wang](https://www.cs.cmu.edu/~haohanw/), [Bhiksha Raj](https://www.lti.cs.cmu.edu/news/bhiksha-raj-named-ieee-fellow), and [Eric P. Xing](http://www.cs.cmu.edu/~epxing/) at [Carnegie Mellon University](https://ai.cs.cmu.edu) wrote a more advanced perspective on the evolution of Deep Learning. This 2017 paper requires a bit more math background. I especially appreciate that section 7 explores optimization of neural networks beyond "vanilla backpropagation", esp. techniques such as rprop, AdaGrad, AdaDelta, and Adaptive Moment Estimation (ADAM). It also has a concise description of the powerful [dropout regularization technique](https://en.wikipedia.org/wiki/Convolutional_neural_network#Dropout). The original dropout paper by Geoffrey Hinton, Nitish Srivastava, et alia just appeared five years ago and is already one of the most cited deep learning papers–[see PDF here](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf).
